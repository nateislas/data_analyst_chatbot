"""
Data Analyst Workflow (Planning Version)
Combines the LlamaIndex Planning Workflow pattern with Pandas Code Execution.
"""

import os
import pandas as pd
import asyncio
import threading
from typing import Optional, List, Dict, Any, Union
from pathlib import Path
from pydantic import BaseModel, Field
from llama_index.core import Settings, PromptTemplate
from llama_index.llms.google_genai import GoogleGenAI
from workflows import Workflow, Context, step
from workflows.events import Event, StartEvent, StopEvent
from load_data import get_csv_metadata, generate_dataset_description
import ast
import re
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import tempfile
import uuid
import json
from datetime import datetime

# =============================================================================
# Sync Runner (Pattern from research_chatbot)
# =============================================================================

class WorkflowRunner:
    """
    Thread-safe runner for executing async workflows from synchronous contexts.
    Uses a dedicated background thread with its own event loop.
    """
    
    def __init__(self):
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._thread: Optional[threading.Thread] = None
        self._lock = threading.Lock()
        self._started = False
    
    def _run_event_loop(self):
        """Run the event loop in a background thread."""
        # Force a standard asyncio event loop (not uvloop) to avoid conflicts
        # Temporarily set policy to default (non-uvloop) for this thread
        original_policy = asyncio.get_event_loop_policy()
        try:
            # Use the default policy which creates standard asyncio loops
            from asyncio import DefaultEventLoopPolicy
            asyncio.set_event_loop_policy(DefaultEventLoopPolicy())
            self._loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._loop)
        finally:
            # Restore original policy (doesn't affect our loop)
            asyncio.set_event_loop_policy(original_policy)
        
        self._loop.run_forever()
    
    def start(self):
        """Start the background event loop thread."""
        with self._lock:
            if self._started:
                return
            self._thread = threading.Thread(target=self._run_event_loop, daemon=True)
            self._thread.start()
            # Wait for loop to be ready
            while self._loop is None:
                threading.Event().wait(0.01)
            self._started = True
    
    def run_coroutine(self, coro):
        """Run a coroutine in the background event loop thread-safely."""
        if not self._started:
            self.start()
        if self._loop is None:
            raise RuntimeError("Event loop not initialized")
        future = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return future.result()

_runner = WorkflowRunner()

# =============================================================================
# Event Definitions (Tutorial Pattern)
# =============================================================================

class QueryPlanItem(Event):
    """A single analytical step in the query plan."""
    name: str  # e.g., "data_analysis_tool"
    query: str # e.g., "Find the top 3 genres by count"

class QueryPlan(BaseModel):
    """The full execution plan generated by the LLM."""
    items: List[QueryPlanItem] = Field(description="List of analysis steps to execute")

class QueryPlanItemResult(Event):
    """The outcome of a single code execution step."""
    query: str
    result: str
    plot_paths: Optional[List[str]] = None  # Paths to generated plot images

class ExecutedPlanEvent(Event):
    """Aggregated results from all plan items, ready for decision/final answer."""
    result: str
    plot_paths: Optional[List[str]] = None  # All plot paths from all steps

# =============================================================================
# Workflow Implementation
# =============================================================================

class DataAnalystWorkflow(Workflow):
    """
    Workflow that PLANS and EXECUTES analytical queries.
    Uses the pattern from: https://developers.llamaindex.ai/python/examples/workflow/planning_workflow/
    """
    
    def __init__(
        self, 
        google_api_key: Optional[str] = None,
        gemini_model: str = "gemini-2.5-flash-preview-09-2025",
        timeout: int = 300, 
        verbose: bool = False,
        log_file_path: Optional[str] = None,
        max_iterations: int = 3,
        session_folder: Optional[str] = None,
        **kwargs
    ):
        super().__init__(timeout=timeout, verbose=verbose, **kwargs)
        self.api_key = google_api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY is required.")
            
        self.llm = GoogleGenAI(api_key=self.api_key, model=gemini_model, temperature=0.1)
        Settings.llm = self.llm
        self.log_file_path = log_file_path
        self.max_iterations = max_iterations
        self.session_folder = session_folder

        # Wrap prompts in PromptTemplate for Gemini compatibility
        self.planning_prompt = PromptTemplate(
            "You are a master data analyst planner. You have access to a dataset with the following information:\n"
            "Dataset Description: {description}\n"
            "Available Columns: {columns}\n\n"
            "{chat_history}"
            "Given a user's query, break it down into a plan of sub-queries that can be answered "
            "by running Python/Pandas code on the DataFrame 'df'.\n\n"
            "STRATEGIC PLANNING RULES:\n"
            "1. PONDER THE DOMAIN: Before creating the plan, analyze the 'Dataset Description'. "
            "Is this a financial dataset? A social study? A scientific experiment? "
            "Identify the most interesting questions a human would want to know about THIS specific data.\n"
            "2. TAILORED EDA: If the user asks for 'EDA' or 'understand the data', DO NOT just provide "
            "generic stats. Instead, design a plan that explores the 'Key Analytical Questions' "
            "suggested in the description. For example:\n"
            "   - For Time-Series: Analyze trends over time, seasonality, and growth.\n"
            "   - For Categorical Data: Identify top categories, distributions, and outliers.\n"
            "   - For Social/Demographic: Analyze correlations between groups and outcomes.\n"
            "   - For Financial: Analyze volatility, correlations, and key metrics.\n"
            "3. COLUMN INFERENCE: Always infer actual column names from conceptual terms (e.g., 'wealth' -> 'gdp_per_capita').\n"
            "4. VISUALIZATION: Always include at least 1-2 plotting steps using matplotlib/seaborn if it helps 'understand' the data.\n"
            "5. COMPREHENSIVE PLAN: Generate 3-5 distinct sub-queries in your plan to provide a full picture in one go.\n\n"
            "User query: {query}\n"
            "Create an insightful, domain-specific plan to answer this query using the actual column names available."
        )
        
        self.decision_prompt = PromptTemplate(
            "You are a data analyst reviewing results. \n"
            "Original query: {query}\n"
            "Dataset context: {description}\n"
            "{chat_history}"
            "Current execution results:\n{results}\n\n"
            "Current Progress: This is iteration {iteration} of {max_iterations}.\n\n"
            "DECISION RULES:\n"
            "1. If the results fully answer the query, provide the 'FINAL' answer.\n"
            "2. If this is an exploratory task (like EDA), provide a 'FINAL' summary if you have "
            "provided a good overview of the data, even if more could be done.\n"
            "3. If you have reached the maximum iterations ({max_iterations}), you MUST provide a 'FINAL' "
            "answer based on the information gathered so far.\n"
            "4. Only respond with 'PLAN' if you genuinely lack critical information to answer the user's primary intent."
        )

    @step
    async def prepare_and_plan(
        self, ctx: Context, ev: StartEvent | ExecutedPlanEvent
    ) -> QueryPlanItem | StopEvent | None:
        """Step 1: First time - setup and plan. Subsequent times - decide or re-plan."""
        
        # Load dataset info if first run
        if isinstance(ev, StartEvent):
            file_path = getattr(ev, "file_path", None)
            query = getattr(ev, "query", None)
            if not file_path or not query:
                raise ValueError("StartEvent must include 'file_path' and 'query'")
            
            # Get chat history for multi-turn conversations
            chat_history = getattr(ev, "chat_history", None) or []
            
            # Check if we have cached metadata and description
            cached_metadata = getattr(ev, "cached_metadata", None)
            cached_description = getattr(ev, "cached_description", None)
            
            # Log query start
            self._log("Query Started", {
                "query": query,
                "file_path": file_path,
                "chat_history_length": len(chat_history),
                "using_cached_description": bool(cached_metadata and cached_description)
            })
            
            # Initial Data Prep
            if cached_metadata and cached_description:
                # Use cached data - no LLM call needed!
                metadata = cached_metadata
                description = cached_description
                ctx.write_event_to_stream(Event(msg="Dataset loaded from cache. Generating plan..."))
            else:
                # Generate new description (first time or cache miss)
                metadata = get_csv_metadata(file_path)
                description = await generate_dataset_description(metadata, self.llm)
                ctx.write_event_to_stream(Event(msg="Dataset loaded and interpreted. Generating plan..."))
            
            df = pd.read_csv(file_path)
            
            # Log dataset info
            self._log("Dataset Loaded", {
                "file_path": file_path,
                "num_rows": len(df),
                "num_columns": len(df.columns),
                "columns": list(df.columns),
                "description": description
            })
            
            # Persist state
            await ctx.store.set("df", df)
            await ctx.store.set("description", description)
            await ctx.store.set("original_query", query)
            await ctx.store.set("metadata", metadata)
            await ctx.store.set("chat_history", chat_history)
            await ctx.store.set("iteration_count", 0)
        else:
            # Iterative decision (Tutorial Pattern)
            query = await ctx.store.get("original_query")
            description = await ctx.store.get("description")
            chat_history = await ctx.store.get("chat_history", [])
            iteration_count = await ctx.store.get("iteration_count", 0)
            iteration_count += 1
            await ctx.store.set("iteration_count", iteration_count)
            
            current_results = ev.result
            
            # Format chat history for prompt
            history_str = self._format_chat_history(chat_history)
            
            # Log decision prompt - access template string directly to avoid KeyError
            try:
                template_str = self.decision_prompt.template if hasattr(self.decision_prompt, 'template') else str(self.decision_prompt)
                decision_prompt_text = template_str.format(
                    query=query,
                    description=description,
                    chat_history=history_str,
                    results=current_results,
                    iteration=iteration_count,
                    max_iterations=self.max_iterations
                )
            except (KeyError, AttributeError):
                # Fallback: manually construct the prompt for logging
                decision_prompt_text = (
                    f"You are a data analyst reviewing results.\n"
                    f"Original query: {query}\n"
                    f"Dataset context: {description}\n"
                    f"{history_str}"
                    f"Current execution results:\n{current_results}\n\n"
                    f"Current Progress: This is iteration {iteration_count} of {self.max_iterations}.\n"
                )
            self._log("Decision Making", {
                "query": query,
                "prompt": decision_prompt_text,
                "aggregated_results": current_results,
                "iteration": iteration_count
            })
            
            # If we hit hard limit, force final answer by instructing LLM
            force_final = iteration_count >= self.max_iterations
            
            decision = await self.llm.apredict(
                self.decision_prompt,
                query=query,
                description=description,
                chat_history=history_str,
                results=current_results,
                iteration=iteration_count,
                max_iterations=self.max_iterations
            )
            
            # Log decision response
            self._log("Decision Response", {
                "query": query,
                "decision": decision,
                "will_continue": "PLAN" in decision.upper() and not force_final,
                "iteration": iteration_count
            })
            
            # Stop if the LLM says so OR if we hit the hard limit
            if "PLAN" not in decision.upper() or force_final:
                # Get plot paths from the executed plan event
                plot_paths = getattr(ev, "plot_paths", None)
                
                # If we forced a final answer but LLM tried to plan, 
                # we should probably add a disclaimer, but the prompt rules
                # usually handle this well enough.
                
                # Log final result
                self._log("Query Completed", {
                    "query": query,
                    "final_response": decision,
                    "plot_paths": plot_paths,
                    "iterations": iteration_count,
                    "forced_stop": force_final and "PLAN" in decision.upper()
                })
                
                return StopEvent(result={
                    "response": decision,
                    "metadata": await ctx.store.get("metadata"),
                    "description": description,
                    "plot_paths": plot_paths
                })
            
            ctx.write_event_to_stream(Event(msg=f"Analysis round {iteration_count} insufficient. Refining plan..."))

        # Planning / Re-Planning
        description = await ctx.store.get("description")
        metadata = await ctx.store.get("metadata")
        query = await ctx.store.get("original_query")
        chat_history = await ctx.store.get("chat_history", [])
        columns_str = ", ".join(metadata["columns"])
        
        # Format chat history for prompt
        history_str = self._format_chat_history(chat_history)
        
        # Log planning prompt - access template string directly to avoid KeyError
        # PromptTemplate has a .template attribute with the actual template string
        try:
            template_str = self.planning_prompt.template if hasattr(self.planning_prompt, 'template') else str(self.planning_prompt)
            planning_prompt_text = template_str.format(
                description=description,
                columns=columns_str,
                chat_history=history_str,
                query=query
            )
        except (KeyError, AttributeError):
            # Fallback: manually construct the prompt for logging
            planning_prompt_text = (
                f"You are a master data analyst planner. You have access to a dataset with the following information:\n"
                f"Dataset Description: {description}\n"
                f"Available Columns: {columns_str}\n\n"
                f"{history_str}"
                f"Given a user's query, break it down into a plan of sub-queries that can be answered "
                f"by running Python/Pandas code on the DataFrame 'df'.\n\n"
                f"User query: {query}\n"
            )
        
        self._log("Planning Phase", {
            "query": query,
            "prompt": planning_prompt_text,
            "chat_history": chat_history
        })
        
        query_plan = await self.llm.astructured_predict(
            QueryPlan,
            self.planning_prompt,
            description=description,
            columns=columns_str,
            chat_history=history_str,
            query=query
        )
        
        # Log the generated plan
        plan_items = [{"name": item.name, "query": item.query} for item in query_plan.items]
        self._log("Generated Plan", {
            "query": query,
            "num_steps": len(query_plan.items),
            "plan_items": plan_items
        })
        
        ctx.write_event_to_stream(Event(msg=f"Executing plan: {len(query_plan.items)} step(s)"))
        
        # Dispatch items
        await ctx.store.set("num_items", len(query_plan.items))
        for item in query_plan.items:
            ctx.send_event(item)
        
        return None

    def _format_chat_history(self, chat_history: List[Dict[str, str]]) -> str:
        """Format chat history for inclusion in prompts."""
        if not chat_history:
            return ""
        
        # Format last N messages (keep recent context, avoid token bloat)
        # Keep last 4 messages (2 user + 2 assistant pairs)
        recent_history = chat_history[-4:] if len(chat_history) > 4 else chat_history
        
        history_lines = ["Previous conversation:"]
        for msg in recent_history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                history_lines.append(f"User: {content}")
            elif role == "assistant":
                history_lines.append(f"Assistant: {content}")
        
        return "\n".join(history_lines) + "\n\n"
    
    def _log(self, message: str, data: Optional[Dict[str, Any]] = None):
        """Log message and optional data to file."""
        if not self.log_file_path:
            return
        
        try:
            log_dir = Path(self.log_file_path).parent
            log_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().isoformat()
            log_entry = {
                "timestamp": timestamp,
                "message": message
            }
            if data:
                log_entry["data"] = data
            
            with open(self.log_file_path, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, indent=2, default=str) + "\n\n")
        except Exception as e:
            # Don't fail if logging fails
            pass
    
    async def _execute_pandas_code(self, df: pd.DataFrame, query: str, description: str, columns_str: str) -> Dict[str, Any]:
        """Execute pandas code generation and execution using async LLM methods. Returns dict with 'output' and 'plot_paths'."""
        # Check if query is asking for a plot/visualization
        plot_keywords = ['plot', 'chart', 'graph', 'visualize', 'visualization', 'histogram', 'bar chart', 
                        'line chart', 'scatter', 'pie chart', 'box plot', 'heatmap', 'show me', 'display']
        is_plot_query = any(keyword in query.lower() for keyword in plot_keywords)
        
        instruction_str = (
            f"You are an expert data analyst working with a pandas DataFrame 'df'.\n\n"
            f"Dataset Description: {description}\n\n"
            f"Available Columns: {columns_str}\n\n"
            f"Task: {query}\n\n"
            "IMPORTANT INSTRUCTIONS:\n"
            "1. First, examine the actual column names using df.columns or df.head() to understand the data structure.\n"
            "2. If the query mentions a concept (e.g., 'genre', 'artist', 'price'), infer which actual column name contains that information.\n"
            "3. For example, if asked about 'genre' but there's no exact 'genre' column, look for columns like 'artist_genres', 'track_genre', 'genre_name', etc.\n"
            "4. Use the actual column names that exist in the DataFrame, not the conceptual terms from the query.\n"
        )
        
        if is_plot_query:
            instruction_str += (
                "5. If the user asks for a plot/chart/visualization, create a matplotlib figure using plt.figure() and plot the data.\n"
                "6. CRITICAL: You MUST use the variable 'plot_path' (which is already defined) to save the plot: plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n"
                "7. DO NOT hardcode 'plot.png' or any other path. Always use the 'plot_path' variable provided.\n"
                "8. After saving, close the figure with plt.close().\n"
                "9. Assign a descriptive message to 'output' explaining what was plotted (e.g., 'Created a bar chart showing...').\n"
                "10. You can use matplotlib.pyplot (imported as plt). If seaborn is available, it will be imported as 'sns', but if it's not available, use matplotlib only.\n"
            )
        else:
            instruction_str += (
                "5. Generate ONLY valid Python code that can be executed. The final result must be assigned to the variable 'output'.\n"
            )
        
        instruction_str += (
            "11. Do not include any explanations, markdown code blocks (```python), or comments in the code.\n"
            "12. If you cannot find a relevant column, assign a helpful error message string to 'output' explaining what column might be needed."
        )
        
        # Use async LLM method to generate code
        prompt = f"{instruction_str}\n\nGenerate the Python code to solve this task. Return ONLY the code, no explanations:"
        
        # Log the prompt
        self._log("Code Generation Prompt", {
            "query": query,
            "prompt": prompt,
            "is_plot_query": is_plot_query
        })
        
        response = await self.llm.acomplete(prompt)
        code_str = str(response).strip()
        
        # Log the raw response
        self._log("Code Generation Response (Raw)", {
            "query": query,
            "raw_response": code_str
        })
        
        # Clean up the code - remove markdown blocks if present
        code_str = re.sub(r'```python\s*', '', code_str)
        code_str = re.sub(r'```\s*', '', code_str)
        code_str = code_str.strip()
        
        # Extract code if there's text before/after it
        # Look for lines that look like code (contain assignments, function calls, etc.)
        lines = code_str.split('\n')
        code_lines = []
        in_code_block = False
        for line in lines:
            # Skip explanation lines (usually start with words like "Here", "The", "This", etc.)
            if re.match(r'^(Here|The|This|To|First|Next|Then|Finally|Note|Explanation)', line, re.IGNORECASE):
                continue
            # Skip lines that are just text (no Python syntax)
            if re.search(r'(output\s*=|df\.|pd\.|import|def |if |for |while )', line):
                in_code_block = True
            if in_code_block or re.search(r'[=\(\)\[\]\.]', line):
                code_lines.append(line)
        
        if code_lines:
            code_str = '\n'.join(code_lines).strip()
        
        # Log the cleaned code
        self._log("Code to Execute", {
            "query": query,
            "code": code_str
        })
        
        # Execute the code safely
        try:
            # Use session-specific folder if provided, otherwise fall back to temp directory
            if self.session_folder:
                plot_dir = Path(self.session_folder)
                plot_dir.mkdir(parents=True, exist_ok=True)
            else:
                plot_dir = Path(tempfile.gettempdir()) / "data_analyst_plots"
                plot_dir.mkdir(exist_ok=True)
            
            plot_filename = f"plot_{uuid.uuid4().hex[:8]}.png"
            plot_path = plot_dir / plot_filename
            
            # Create a safe execution environment with plotting libraries
            local_vars = {
                "df": df, 
                "pd": pd,
                "plt": plt,
                "matplotlib": matplotlib,
                "plot_path": str(plot_path)  # Provide plot_path variable to LLM
            }
            
            # Try to import seaborn if available
            seaborn_available = False
            try:
                import seaborn as sns
                local_vars["sns"] = sns
                seaborn_available = True
            except ImportError:
                pass
            
            global_vars = {"__builtins__": __builtins__}
            
            # Execute the code - handle seaborn import errors gracefully
            try:
                exec(code_str, global_vars, local_vars)
            except (ModuleNotFoundError, NameError) as import_error:
                # If seaborn import failed in the code, try to fix it
                if "seaborn" in str(import_error).lower() or "sns" in str(import_error).lower():
                    # Replace seaborn imports and calls with matplotlib equivalents
                    code_str_fixed = code_str.replace("import seaborn as sns", "# seaborn not available, using matplotlib")
                    code_str_fixed = code_str_fixed.replace("sns.lineplot", "plt.plot")
                    code_str_fixed = code_str_fixed.replace("sns.barplot", "plt.bar")
                    code_str_fixed = code_str_fixed.replace("sns.scatterplot", "plt.scatter")
                    code_str_fixed = code_str_fixed.replace("sns.histplot", "plt.hist")
                    code_str_fixed = code_str_fixed.replace("sns.boxplot", "plt.boxplot")
                    # Try executing the fixed code
                    try:
                        exec(code_str_fixed, global_vars, local_vars)
                    except Exception as e2:
                        raise import_error  # Re-raise original error if fix didn't work
                else:
                    raise  # Re-raise if it's not a seaborn error
            
            # Check for generated plots - only in session folder
            plot_paths = []
            
            # 1. Check the session folder path (UUID-based)
            if plot_path.exists():
                plot_paths.append(str(plot_path))
            
            # 2. Check session folder for 'plot.png' (fallback for LLM that hardcodes path)
            session_plot_png = plot_dir / "plot.png"
            if session_plot_png.exists() and str(session_plot_png) not in plot_paths:
                plot_paths.append(str(session_plot_png))
            
            # 3. Check if plt has any figures open (in case code didn't save)
            if plt.get_fignums():
                # Save any open figures
                for i, fig_num in enumerate(plt.get_fignums()):
                    fig = plt.figure(fig_num)
                    if i == 0:
                        # Use the main plot path
                        fig.savefig(plot_path, dpi=100, bbox_inches='tight')
                        if plot_path.exists() and str(plot_path) not in plot_paths:
                            plot_paths.append(str(plot_path))
                    else:
                        # Save additional figures with different names
                        additional_plot_path = plot_dir / f"plot_{uuid.uuid4().hex[:8]}.png"
                        fig.savefig(additional_plot_path, dpi=100, bbox_inches='tight')
                        if additional_plot_path.exists():
                            plot_paths.append(str(additional_plot_path))
                    plt.close(fig)
            
            # 4. Filter and return unique paths for THIS execution
            plot_paths = list(dict.fromkeys(plot_paths)) # Remove duplicates while preserving order
            
            # Get the output
            if "output" in local_vars:
                output = local_vars["output"]
                # Convert to string representation
                if isinstance(output, pd.Series):
                    output_str = str(output.to_dict())
                elif isinstance(output, pd.DataFrame):
                    output_str = output.to_string()
                else:
                    output_str = str(output)
                
                result = {
                    "output": output_str,
                    "plot_paths": plot_paths if plot_paths else None
                }
                
                # Log successful execution
                self._log("Code Execution Success", {
                    "query": query,
                    "output": output_str,
                    "plot_paths": plot_paths,
                    "output_type": type(output).__name__
                })
                
                return result
            else:
                error_msg = "Error: Code did not assign a value to 'output'"
                self._log("Code Execution Error", {
                    "query": query,
                    "error": error_msg,
                    "code": code_str
                })
                return {
                    "output": error_msg,
                    "plot_paths": plot_paths if plot_paths else None
                }
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            self._log("Code Execution Exception", {
                "query": query,
                "error": str(e),
                "traceback": error_trace,
                "code": code_str
            })
            return {
                "output": f"Execution error: {str(e)}",
                "plot_paths": None
            }

    @step(num_workers=4)
    async def execute_analysis_step(
        self, ctx: Context, ev: QueryPlanItem
    ) -> QueryPlanItemResult:
        """Step 2: Execute code for a single plan item (Tutorial Pattern)."""
        df = await ctx.store.get("df")
        description = await ctx.store.get("description")
        metadata = await ctx.store.get("metadata")
        columns_str = ", ".join(metadata["columns"])
        
        ctx.write_event_to_stream(Event(msg=f"Running analysis: {ev.query}"))
        try:
            result_dict = await self._execute_pandas_code(df, ev.query, description, columns_str)
            
            # Log step result
            self._log("Analysis Step Result", {
                "step_query": ev.query,
                "result": result_dict["output"],
                "plot_paths": result_dict.get("plot_paths"),
                "success": True
            })
            
            return QueryPlanItemResult(
                query=ev.query, 
                result=result_dict["output"],
                plot_paths=result_dict.get("plot_paths")
            )
        except Exception as e:
            import traceback
            error_msg = f"Step failed: {str(e)}"
            error_trace = traceback.format_exc()
            
            # Log step error
            self._log("Analysis Step Error", {
                "step_query": ev.query,
                "error": error_msg,
                "traceback": error_trace,
                "success": False
            })
            
            ctx.write_event_to_stream(Event(msg=f"Step failed: {error_msg}"))
            return QueryPlanItemResult(query=ev.query, result=error_msg, plot_paths=None)

    @step
    async def aggregate_results(
        self, ctx: Context, ev: QueryPlanItemResult
    ) -> ExecutedPlanEvent:
        """Step 3: Collect all results before moving to decision (Tutorial Pattern)."""
        num_items = await ctx.store.get("num_items")
        results = ctx.collect_events(ev, [QueryPlanItemResult] * num_items)
        
        if results is None:
            return None
        
        # Aggregate text results
        aggregated_result = "\n".join([f"Step: {r.query}\nResult: {r.result}\n" for r in results])
        
        # Collect all plot paths
        all_plot_paths = []
        for r in results:
            if r.plot_paths:
                all_plot_paths.extend(r.plot_paths)
        
        # Log aggregated results
        self._log("Aggregated Results", {
            "num_steps": len(results),
            "aggregated_result": aggregated_result,
            "plot_paths": all_plot_paths,
            "step_results": [{"query": r.query, "result": r.result, "plot_paths": r.plot_paths} for r in results]
        })
        
        return ExecutedPlanEvent(
            result=aggregated_result,
            plot_paths=all_plot_paths if all_plot_paths else None
        )
